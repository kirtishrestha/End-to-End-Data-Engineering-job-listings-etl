What You Did Well (The Strengths)
Decoupling of Processes: By separating extraction, loading, and transformation into different stages and scripts, you've created a modular system. If the final Spark transformation fails, you don't need to re-extract the data from the API. You can simply debug the Spark script and re-run it using the already-saved CSV data. This is a huge advantage.
Creation of a Durable Staging Layer: Loading the raw data into both PostgreSQL and a Snowflake landing schema is an excellent practice. This creates a durable, queryable, and backed-up version of the source data. If there are ever questions about the transformed data, you can always go back to this "source of truth" in the landing tables to validate it.
Logical Task Sequencing: Your pipeline follows a logical progression:
Get the data.
Stage the data in a transactional database (Postgres).
Move the staged data to the cloud data warehouse's landing zone (Snowflake).
Perform heavy-duty, scalable transformations (Spark).
Load the final, clean model for analytics (Snowflake Star Schema).

Proposed Refined Flow:

Extract: API -> Raw Data (in memory or temporary file).
Load: Raw Data -> PostgreSQL landing schema.
Transfer: PostgreSQL landing schema -> Snowflake landing schema.
Transform: Spark job reads directly from the Snowflake landing schema, performs the transformations, and writes back to the Snowflake Star Schema.